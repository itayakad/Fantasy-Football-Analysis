{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:24px\"><b>Predicting Fantasy Football Stars</b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I love fantasy football. In general, I love sports, but there’s something about sitting down for an entire day on Sundays and watching football with your league mates that cannot be beat. Compared to other sports, fantasy football is easily the best type of fantasy sport. Basketball and baseball have too many games in a week to constantly be updating and changing your line up, and soccer has too little actual metrics to use to determine what fantasy points are worth. However, football has the perfect mix of frequency and statistical measurement. Games aren’t on too frequently, but not too scarcely either. Points are judged by yards and touchdowns, so one play can literally make or break your week. Overall, fantasy football is very exciting and fun, and I’m hoping that I can somehow predict who I should draft next year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Cleaning Data</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost, I need a data set to work with. Thankfully, Funk Monarch on Kaggle posted a huge data set containing every important offensive metric for every player from every week since 2012. This was a lot of data to sort through, but ultimately, through the help of SQL queries and ChatGPT, I managed to isolate what I determined to be the important metrics for flex positions (wide receivers, running backs, and tight ends) in one spreadsheet, and quarterbacks in another spreadsheet. Now, I can begin building a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Building The Model (Flex Players)</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build the actual model, I transitioned from MySQL to VS Code to use python. Python has various libraries that make building predictive models much, much easier. To start, I ran a correlation test to see which of the categorical values in my dataset had the highest correlation to ppr_ppg (PPR Points Per Game). Everything in the data will be catered towards PPR scoring, so I apologize in advance to all of those in standard and half-PPR scoring leagues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flex_data_path = '/Users/itayakad/Desktop/Github Projects/FantasyFootballPredictor/Data/oy_flex.csv'\n",
    "flex_data = pd.read_csv(flex_data_path)\n",
    "numeric_columns = flex_data.select_dtypes(include=['number'])\n",
    "correlation = numeric_columns.corr()\n",
    "ppr_ppg_correlation = correlation['ppr_ppg'].sort_values(ascending=False).head(10)\n",
    "print(ppr_ppg_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the test, I decided to train my model based on the values YPG (yards per game), total yards, total touchdowns, receiving yards after catch, receptions, and games played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_features = ['ypg','total_yards','total_tds','receiving_yards_after_catch','receptions','games']\n",
    "X = numeric_columns[top_features]\n",
    "y = numeric_columns['ppr_ppg']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Testing the Algorithm</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our model (which has pretty good MSE and R^2 values), I want to test how strong it is. To do this, I used the model to predict who the top 20 players each year in PPR PPG will be and compared it with a list of the actual top 20 players in total PPR points for that year. I then gave it an accuracy score, which awarded 2 points to model if it got the right person in the right position that they finished, 1 point if it predited a person to be in the top 20, but in the wrong position, and 0 if it completely missed. I then compared this with a control model that predicts the top 20 based on who the top 20 in PPR PPG were the year before. Essentially, this allows me to see how much more accurate my model is compared to blindly copy and pasting the top 20 players in PPR PPG from the year prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_year(train_year):\n",
    "    data_train = flex_data[flex_data['season'] == train_year]\n",
    "    data_actual = flex_data[flex_data['season'] == (train_year + 1)]\n",
    "    numeric_columns_train = data_train.select_dtypes(include=['number'])\n",
    "\n",
    "    X_train = numeric_columns_train[top_features]\n",
    "    y_train = numeric_columns_train['ppr_ppg']\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    data_predict = data_train.copy()\n",
    "    data_predict['predicted_ppr_ppg'] = model.predict(X_train)\n",
    "\n",
    "    top_20_predicted = data_predict[['name', 'predicted_ppr_ppg']].sort_values(by='predicted_ppr_ppg', ascending=False).head(20).reset_index(drop=True)\n",
    "    top_20_predicted['predicted_rank'] = range(1, 21)\n",
    "\n",
    "    top_20_actual = data_actual[['name', 'fantasy_points_ppr']].sort_values(by='fantasy_points_ppr', ascending=False).head(20).reset_index(drop=True)\n",
    "    top_20_actual['actual_rank'] = range(1, 21)\n",
    "\n",
    "    score = accuracy_test(top_20_actual, top_20_predicted)\n",
    "\n",
    "    combined_results = pd.DataFrame({\n",
    "        'Rank': range(1, 21),\n",
    "        'Projected Leaders': top_20_predicted['name'],\n",
    "        'Projected PPR PPG': top_20_predicted['predicted_ppr_ppg'],\n",
    "        '': range(1,21),\n",
    "        'Actual Leaders': top_20_actual['name'],\n",
    "        'Actual PPR Total': top_20_actual['fantasy_points_ppr']\n",
    "    })\n",
    "    \n",
    "    print(f\"Projected vs Actual Leaders for the {train_year + 1} Season based on {train_year} Stats:\")\n",
    "    print(combined_results)\n",
    "    print(f\"Accuracy Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control(train_year):\n",
    "    data_train = flex_data[flex_data['season'] == train_year]\n",
    "    data_actual = flex_data[flex_data['season'] == (train_year + 1)]\n",
    "    \n",
    "    top_20_actual = data_actual[['name', 'fantasy_points_ppr']].sort_values(by='fantasy_points_ppr', ascending=False).head(20).reset_index(drop=True)\n",
    "    top_20_actual['actual_rank'] = range(1, 21)\n",
    "\n",
    "    control_predictions = data_train[['name', 'ppr_ppg']].sort_values(by='ppr_ppg', ascending=False).head(20).reset_index(drop=True)\n",
    "    control_predictions['predicted_rank'] = range(1, 21)\n",
    "    \n",
    "    score = accuracy_test(top_20_actual, control_predictions)\n",
    "    print(f\"Accuracy Score (Control): {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_test(actual, predicted):\n",
    "    score = 0\n",
    "    predicted_ranks = {row['name']: row['predicted_rank'] for index, row in predicted.iterrows()}\n",
    "    actual_ranks = {row['name']: row['actual_rank'] for index, row in actual.iterrows()}\n",
    "    for player in actual_ranks:\n",
    "        if player in predicted_ranks:\n",
    "            if predicted_ranks[player] == actual_ranks[player]:\n",
    "                score += 2  # Exact rank match\n",
    "            else:\n",
    "                score += 1  # Player is in the top 20 but not the exact rank\n",
    "\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data():\n",
    "    for i in range(3,14):\n",
    "        predict_next_year(2010+i)\n",
    "        control(2010+i)\n",
    "\n",
    "test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Improving the Model</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results, my model had a combined accuracy score of 88 vs the control's score of 84. While this is only a 4.7% change, it shows that using my model has improved results over simply basing it off of last year's rankings. To try and improve this model, I found a data set on Kaggle from Nick Cantalupa that has every important team metric from the last 20 years. After cleaning and fixing some inconsistent variables, I joined my 2 data sets together to get one big flex player data set.\n",
    "\n",
    "I decided to add how many points the player's team scored that season ('points') and how many offensive plays that team had ('plays_offense') as 2 new predicters because an NFL player cannot be good in fantasy if his team is not putting up points. That's not to say that a team cannot be bad with a great fantasy NFl player, though. A bad team can have a top 5 fantasy player and put up 20-30 points each week, but if they are allowing 30-40 points each week, then they are a bad football team. Fortunatley, I do not care about whether or not the team is good or bad, I just care if they put up points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flex_data_path = '/Users/itayakad/Desktop/Github Projects/FantasyFootballPredictor/Data/flex_team.csv'\n",
    "flex_data = pd.read_csv(flex_data_path)\n",
    "numeric_columns = flex_data.select_dtypes(include=['number'])\n",
    "\n",
    "top_features = ['ypg','total_yards','total_tds','receiving_yards_after_catch','receptions','games','points','plays_offense']\n",
    "X = numeric_columns[top_features]\n",
    "y = numeric_columns['ppr_ppg']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding 'points' to my model actually lowered the MSE by 0.06, and given the size of the data set, this is very good. Now, I will check it against the existing data to see how it performs against the control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This upgraded model performed worse than the previous one (Accuracy score of 87), despite having a lower MSE. What this is telling me is that we are using the right statistical measures to make our predictions, but are getting \"unlucky\", and the source of our unluckiness is due to something \"season altering injuries\". For example, the model predicted Nick Chubb to be a top player in 2023, and he looked like he was going to be after Weeks 1 and 2. Then, boom. He destroyed his knee in Week 3 and was out for the rest of the year.\n",
    "\n",
    "To confirm my suspcisions, I adjusted the test_data() method to compare against PPR PPG leaders instead of Total PPR Points leaders. If the accuracy score is higher, then this tells me that the model is accuratley predicting the best players in PPR formats, without accounting for the chance that they miss an extended period of time due to injuries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_year(train_year):\n",
    "    data_train = flex_data[flex_data['season'] == train_year]\n",
    "    data_actual = flex_data[flex_data['season'] == (train_year + 1)]\n",
    "    numeric_columns_train = data_train.select_dtypes(include=['number'])\n",
    "\n",
    "    X_train = numeric_columns_train[top_features]\n",
    "    y_train = numeric_columns_train['ppr_ppg']\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    data_predict = data_train.copy()\n",
    "    data_predict['predicted_ppr_ppg'] = model.predict(X_train)\n",
    "\n",
    "    top_20_predicted = data_predict[['name', 'predicted_ppr_ppg']].sort_values(by='predicted_ppr_ppg', ascending=False).head(20).reset_index(drop=True)\n",
    "    top_20_predicted['predicted_rank'] = range(1, 21)\n",
    "\n",
    "    top_20_actual = data_actual[['name', 'ppr_ppg']].sort_values(by='ppr_ppg', ascending=False).head(20).reset_index(drop=True)\n",
    "    top_20_actual['actual_rank'] = range(1, 21)\n",
    "\n",
    "    score = accuracy_test(top_20_actual, top_20_predicted)\n",
    "\n",
    "    combined_results = pd.DataFrame({\n",
    "        'Rank': range(1, 21),\n",
    "        'Projected Leaders': top_20_predicted['name'],\n",
    "        'Projected PPR PPG': top_20_predicted['predicted_ppr_ppg'],\n",
    "        '': range(1,21),\n",
    "        'Actual Leaders': top_20_actual['name'],\n",
    "        'Actual PPR PPG': top_20_actual['ppr_ppg']\n",
    "    })\n",
    "    \n",
    "    print(f\"Projected vs Actual Leaders for the {train_year + 1} Season based on {train_year} Stats:\")\n",
    "    print(combined_results)\n",
    "    print(f\"Accuracy Score: {score}\")\n",
    "\n",
    "test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing the model to PPR PPG instead of Total PPR Points, the model gains an additional 3 points of accuracy (90 total). This confirms my suspicions that injuries are indeed impacting the accuracy of this model. But how do we account for injuries, or more specifically, the <i>potential</i> for injuries?\n",
    "\n",
    "First, I want to see if there's any statistical measures that may correlate to a player missing significant time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the flex data\n",
    "flex_data = pd.read_csv('/Users/itayakad/Desktop/Github Projects/FantasyFootballPredictor/Data/flex_team.csv')\n",
    "\n",
    "# Create a target variable for playing very few games (e.g., less than 10 games)\n",
    "flex_data['games_next_season'] = flex_data.groupby('name')['games'].shift(-1)\n",
    "flex_data['few_games_next_season'] = flex_data['games_next_season'].apply(lambda x: 1 if x < 10 else 0)\n",
    "\n",
    "# Drop rows where next season data is not available\n",
    "flex_data = flex_data.dropna(subset=['games_next_season'])\n",
    "\n",
    "# Select only numeric columns for features\n",
    "numeric_columns = flex_data.select_dtypes(include=['number']).drop(columns=['games_next_season'])\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = numeric_columns.corrwith(flex_data['few_games_next_season'])\n",
    "print(correlation.sort_values(ascending=False).head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This correlation test tells me that out of the statistical measures availible, poor play like turning the ball over, throwing interceptions, and overall losing tend to result in a reduced number of games played the following season. Using this information, I will create a model that attempts to rank how likely a player is to being injured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Load the flex data\n",
    "flex_data = pd.read_csv('/Users/itayakad/Desktop/Github Projects/FantasyFootballPredictor/Data/flex_team.csv')\n",
    "\n",
    "# Create a target variable for playing very few games (e.g., less than 10 games)\n",
    "flex_data['games_next_season'] = flex_data.groupby('name')['games'].shift(-1)\n",
    "flex_data['few_games_next_season'] = flex_data['games_next_season'].apply(lambda x: 1 if x < 10 else 0)\n",
    "\n",
    "# Calculate the most correlated features for the model\n",
    "top_features = ['losses', 'pass_int', 'turnovers', 'turnover_pct', 'fumbles_lost', 'years_played']\n",
    "X = flex_data[top_features]\n",
    "y = flex_data['few_games_next_season']\n",
    "\n",
    "# Handle missing data: Fill NaN values with the average of the column\n",
    "X = X.fillna(X.mean())\n",
    "y = y.fillna(0)  # Filling target variable NaNs with 0, indicating not missing games\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a logistic regression model (as we are predicting a binary outcome)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities\n",
    "flex_data['injury_prob'] = model.predict_proba(X)[:, 1] * 100\n",
    "\n",
    "# Handle missing data: Assign the average probability to players without prior year data\n",
    "average_prob = flex_data['injury_prob'].mean()\n",
    "flex_data['injury_prob'] = flex_data['injury_prob'].fillna(average_prob)\n",
    "\n",
    "# Create the 'safe_prob' column as the inverse of 'injury_prob'\n",
    "flex_data['safe_prob'] = 100 - flex_data['injury_prob']\n",
    "\n",
    "# Print the updated dataframe with the new columns for the 2023 season\n",
    "flex_data_2023 = flex_data[flex_data['season'] == 2023]\n",
    "sorted_flex_data = flex_data_2023.sort_values(by='injury_prob', ascending=False)\n",
    "print(sorted_flex_data[['name', 'season', 'injury_prob', 'safe_prob']].head())\n",
    "\n",
    "# Save the updated dataframe to a new CSV file\n",
    "flex_data.to_csv('/Users/itayakad/Desktop/Github Projects/FantasyFootballPredictor/Data/flex_team_injury.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the 'safe prob' (inverse of 'injury prob') category, we can use it to try and see if it improves the model by predicting who will be subject to injury."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flex_data_path = '/Users/itayakad/Desktop/Github Projects/FantasyFootballPredictor/Data/flex_team_injury.csv'\n",
    "flex_data = pd.read_csv(flex_data_path)\n",
    "numeric_columns = flex_data.select_dtypes(include=['number'])\n",
    "\n",
    "top_features = ['ypg','total_yards','total_tds','receiving_yards_after_catch','receptions','games','points','plays_offense','safe_prob']\n",
    "X = numeric_columns[top_features]\n",
    "y = numeric_columns['ppr_ppg']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new model, which includes injury probability, improves the model by 1 point, moving it to a total score of 91. While this is a very minor improvement, it still shows how model continues to improve and learn with every new addition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
